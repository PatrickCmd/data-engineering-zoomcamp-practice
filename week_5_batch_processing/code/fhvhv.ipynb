{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local spark session\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"fhvhv\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.1'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- originating_base_num: string (nullable = true)\n",
      " |-- request_datetime: timestamp (nullable = true)\n",
      " |-- on_scene_datetime: timestamp (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- trip_time: long (nullable = true)\n",
      " |-- base_passenger_fare: double (nullable = true)\n",
      " |-- tolls: double (nullable = true)\n",
      " |-- bcf: double (nullable = true)\n",
      " |-- sales_tax: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- tips: double (nullable = true)\n",
      " |-- driver_pay: double (nullable = true)\n",
      " |-- shared_request_flag: string (nullable = true)\n",
      " |-- shared_match_flag: string (nullable = true)\n",
      " |-- access_a_ride_flag: string (nullable = true)\n",
      " |-- wav_request_flag: string (nullable = true)\n",
      " |-- wav_match_flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .parquet(\"data/raw/fhvhv/*/*\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"data/pq/fhvhv/2021/02/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhvhv = spark.read.parquet(\"data/pq/fhvhv/*/*/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hvfhs_license_num',\n",
       " 'dispatching_base_num',\n",
       " 'originating_base_num',\n",
       " 'request_datetime',\n",
       " 'on_scene_datetime',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'trip_miles',\n",
       " 'trip_time',\n",
       " 'base_passenger_fare',\n",
       " 'tolls',\n",
       " 'bcf',\n",
       " 'sales_tax',\n",
       " 'congestion_surcharge',\n",
       " 'airport_fee',\n",
       " 'tips',\n",
       " 'driver_pay',\n",
       " 'shared_request_flag',\n",
       " 'shared_match_flag',\n",
       " 'access_a_ride_flag',\n",
       " 'wav_request_flag',\n",
       " 'wav_match_flag']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhvhv.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhvhv.createOrReplaceTempView(\"fhvhv_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+\n",
      "|dispatching_base_num|   dropoff_datetime|    pickup_datetime|trip_time|       trip_duration|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+\n",
      "|              B02882|2021-02-15 00:07:37|2021-02-14 23:41:50|     1547|INTERVAL '0 00:25...|\n",
      "|              B02871|2021-02-20 18:42:07|2021-02-20 18:20:32|     1295|INTERVAL '0 00:21...|\n",
      "|              B02882|2021-02-27 21:58:41|2021-02-27 21:44:02|      879|INTERVAL '0 00:14...|\n",
      "|              B02887|2021-02-06 15:18:50|2021-02-06 15:10:36|      494|INTERVAL '0 00:08...|\n",
      "|              B02864|2021-02-27 08:54:41|2021-02-27 08:39:02|      939|INTERVAL '0 00:15...|\n",
      "|              B02875|2021-02-05 09:05:10|2021-02-05 08:50:44|      866|INTERVAL '0 00:14...|\n",
      "|              B02765|2021-02-24 21:14:18|2021-02-24 20:56:55|     1043|INTERVAL '0 00:17...|\n",
      "|              B02764|2021-02-28 12:45:26|2021-02-28 12:33:38|      708|INTERVAL '0 00:11...|\n",
      "|              B02864|2021-02-13 16:35:47|2021-02-13 16:24:32|      675|INTERVAL '0 00:11...|\n",
      "|              B02510|2021-02-09 19:09:40|2021-02-09 18:57:54|      706|INTERVAL '0 00:11...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    dispatching_base_num,\n",
    "    dropoff_datetime,\n",
    "    pickup_datetime,\n",
    "    trip_time,\n",
    "    (to_timestamp(dropoff_datetime) - to_timestamp(pickup_datetime)) AS trip_duration\n",
    "FROM \n",
    "    fhvhv_data\n",
    "LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|feb_15_trips|\n",
      "+------------+\n",
      "|      392133|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_query = \"\"\"\n",
    "SELECT \n",
    "    COUNT(*) AS feb_15_trips\n",
    "FROM\n",
    "    fhvhv_data\n",
    "WHERE \n",
    "    TO_DATE(pickup_datetime) == '2021-02-15'\n",
    "\"\"\"\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1547\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dropoff_datetime = datetime.strptime(\"2021-02-15 00:07:37\", \"%Y-%m-%d %H:%M:%S\")\n",
    "pickup_datetime = datetime.strptime(\"2021-02-14 23:41:50\", \"%Y-%m-%d %H:%M:%S\")\n",
    "diff = dropoff_datetime - pickup_datetime\n",
    "print(round(diff.total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+---------+--------------------+\n",
      "|   dropoff_datetime|    pickup_datetime|trip_time|       trip_duration|\n",
      "+-------------------+-------------------+---------+--------------------+\n",
      "|2021-02-12 13:39:44|2021-02-11 16:40:44|    75540|INTERVAL '0 20:59...|\n",
      "|2021-02-18 10:48:34|2021-02-17 18:54:53|    57220|INTERVAL '0 15:53...|\n",
      "|2021-02-21 03:22:14|2021-02-20 15:08:15|    44038|INTERVAL '0 12:13...|\n",
      "|2021-02-04 10:41:58|2021-02-03 23:24:25|    40653|INTERVAL '0 11:17...|\n",
      "|2021-02-20 12:44:01|2021-02-20 02:17:44|    37577|INTERVAL '0 10:26...|\n",
      "|2021-02-26 05:57:05|2021-02-25 20:13:35|    35010|INTERVAL '0 09:43...|\n",
      "|2021-02-20 14:16:19|2021-02-20 04:36:13|    34806|INTERVAL '0 09:40...|\n",
      "|2021-02-19 04:01:11|2021-02-18 18:24:19|    34612|INTERVAL '0 09:36...|\n",
      "|2021-02-18 14:07:15|2021-02-18 04:31:20|    34555|INTERVAL '0 09:35...|\n",
      "|2021-02-11 09:21:08|2021-02-10 23:51:39|        0|INTERVAL '0 09:29...|\n",
      "+-------------------+-------------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    dropoff_datetime,\n",
    "    pickup_datetime,\n",
    "    trip_time,\n",
    "    (to_timestamp(dropoff_datetime) - to_timestamp(pickup_datetime)) AS trip_duration\n",
    "FROM \n",
    "    fhvhv_data\n",
    "ORDER BY 4 DESC\n",
    "LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|pickup_date|         duration|\n",
      "+-----------+-----------------+\n",
      "| 2021-02-11|           1259.0|\n",
      "| 2021-02-17|953.6833333333333|\n",
      "| 2021-02-20|733.9833333333333|\n",
      "| 2021-02-03|           677.55|\n",
      "| 2021-02-25|            583.5|\n",
      "| 2021-02-18|576.8666666666667|\n",
      "| 2021-02-10|569.4833333333333|\n",
      "| 2021-02-21|           537.05|\n",
      "| 2021-02-09|534.7833333333333|\n",
      "| 2021-02-06|524.1166666666667|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    to_date(pickup_datetime) AS pickup_date,\n",
    "    MAX((CAST(dropoff_datetime AS LONG) - CAST(pickup_datetime AS LONG)) / 60) AS duration\n",
    "FROM \n",
    "    fhvhv_data\n",
    "GROUP BY\n",
    "    1\n",
    "ORDER BY\n",
    "    2 DESC\n",
    "LIMIT 10;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.parquet('zones/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones.createOrReplaceTempView('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|          pu_do_pair|count(1)|\n",
      "+--------------------+--------+\n",
      "|East New York / E...|   45041|\n",
      "|Borough Park / Bo...|   37329|\n",
      "| Canarsie / Canarsie|   28026|\n",
      "|Crown Heights Nor...|   25976|\n",
      "|Bay Ridge / Bay R...|   17934|\n",
      "|Jackson Heights /...|   14688|\n",
      "|   Astoria / Astoria|   14688|\n",
      "|Central Harlem No...|   14481|\n",
      "|Bushwick South / ...|   14424|\n",
      "|Flatbush/Ditmas P...|   13976|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    CONCAT(pul.Zone, ' / ', dol.Zone) AS pu_do_pair,\n",
    "    COUNT(1)\n",
    "FROM \n",
    "    fhvhv_data fhv LEFT JOIN zones pul ON fhv.PULocationID = pul.LocationID\n",
    "                      LEFT JOIN zones dol ON fhv.DOLocationID = dol.LocationID\n",
    "GROUP BY \n",
    "    1\n",
    "ORDER BY\n",
    "    2 DESC\n",
    "LIMIT 10;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
